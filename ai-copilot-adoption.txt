
================================================================================
File: .env.example
Size: 0 B
================================================================================



================================================================================
File: .flake8
Size: 129 B
================================================================================

# .flake8
[flake8]
max-line-length = 88
extend-ignore = E203
exclude = 
    .git,
    __pycache__,
    venv,
    .venv,
    docs/

================================================================================
File: .github/workflows/ci.yml
Size: 917 B
================================================================================

name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest
          if [ -f app/requirements.txt ]; then pip install -r app/requirements.txt; fi

      - name: Lint with flake8
        run: flake8 .

      - name: Run unit tests
        run: |
          pytest tests/

  docker-build:
    runs-on: ubuntu-latest
    needs: lint-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Build Backend Image
        run: docker build -t ai-copilot-backend:latest ./app


================================================================================
File: .gitignore
Size: 247 B
================================================================================

# Python
__pycache__/
*.py[cod]
*$py.class
.venv
venv/
ENV/

# Secrets and Environment
.env
.env.local

# IDEs
.vscode/
.idea/

# OS
.DS_Store

# Vector DB / Local data
data/index_storage/
*.csv

# ML / LLM models
models/
*.gguf
*.bin
*.pt
*.onnx


================================================================================
File: LICENSE
Size: 1.07 kB
================================================================================

MIT License

Copyright (c) 2026 Nikolaspc

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
File: ai-copilot-adoption.txt
Size: 8.21 kB
================================================================================


================================================================================
File: .env.example
Size: 0 B
================================================================================



================================================================================
File: .flake8
Size: 129 B
================================================================================

# .flake8
[flake8]
max-line-length = 88
extend-ignore = E203
exclude = 
    .git,
    __pycache__,
    venv,
    .venv,
    docs/

================================================================================
File: .github/workflows/ci.yml
Size: 1 kB
================================================================================

# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest
          if [ -f app/requirements.txt ]; then pip install -r app/requirements.txt; fi

      - name: Lint with flake8
        run: flake8 .

      - name: Run unit tests
        run: |
          # We'll create a dummy test to ensure this passes
          pytest tests/

  docker-build:
    runs-on: ubuntu-latest
    needs: lint-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Build Backend Image
        run: docker build -t ai-copilot-backend:latest ./app


================================================================================
File: .gitignore
Size: 194 B
================================================================================

# Python
__pycache__/
*.py[cod]
*$py.class
.venv
venv/
ENV/

# Secrets and Environment
.env
.env.local

# IDEs
.vscode/
.idea/

# OS
.DS_Store

# Vector DB / Local data
data/index_storage/
*.csv

================================================================================
File: LICENSE
Size: 1.07 kB
================================================================================

MIT License

Copyright (c) 2026 Nikolaspc

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
File: app/embeddings/ingest.py
Size: 1.31 kB
================================================================================

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Load environment variables (API Key)
load_dotenv()

def ingest_docs():
    # 1. Path to your documents folder
    data_path = "data/"
    documents = []

    # 2. Loop through files in data folder
    for file in os.listdir(data_path):
        if file.endswith(".txt"):
            loader = TextLoader(os.path.join(data_path, file))
            documents.extend(loader.load())
        elif file.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(data_path, file))
            documents.extend(loader.load())

    # 3. Split text into manageable chunks
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)

    # 4. Create embeddings and save to FAISS (Vector DB)
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    
    # 5. Save the index locally
    vectorstore.save_local("faiss_index")
    print("Successfully ingested documents into faiss_index")

if __name__ == "__main__":
    ingest_docs()

================================================================================
File: app/main.py
Size: 240 B
================================================================================

from fastapi import FastAPI

app = FastAPI(title="AI Co-Pilot API")

@app.get("/")
def read_root():
    return {"status": "AI Co-Pilot is online", "version": "0.1.0"}

@app.get("/health")
def health_check():
    return {"status": "healthy"}

================================================================================
File: data/test_info.txt
Size: 213 B
================================================================================

"The AI Co-Pilot Adoption project started in February 2026. Our main goal is to automate internal documentation tasks using FastAPI and OpenAI. We are currently working on the ingestion phase using a MacBook Air."

================================================================================
File: requirements.txt
Size: 0 B
================================================================================



================================================================================
File: test_final.py
Size: 910 B
================================================================================

import torch
from transformers import pipeline
import sys

print("--- System Diagnostics ---")
print(f"Python version: {sys.version.split()[0]}")
print(f"PyTorch version: {torch.__version__}")

try:
    print("\nLoading TinyLlama (1.1B)... this is a very compatible model.")
    # This model is around 700MB, much lighter for your RAM
    pipe = pipeline(
        "text-generation", 
        model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", 
        torch_dtype=torch.float32, 
        device_map="cpu"
    )

    print("\n‚úÖ Success! Local AI is active.")
    
    prompt = "<|system|>\nYou are a helpful assistant.</s>\n<|user|>\nWhat is 2+2?</s>\n<|assistant|>\n"
    
    outputs = pipe(prompt, max_new_tokens=20, do_sample=True, temperature=0.7)
    print(f"\nResponse: {outputs[0]['generated_text'].split('<|assistant|>')[-1].strip()}")

except Exception as e:
    print(f"\n‚ùå Error during execution: {e}")

================================================================================
File: test_local.py
Size: 968 B
================================================================================

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Define a small, efficient model (DeepSeek-1.5B)
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

print("Loading model into RAM... (this might take a few minutes)")

try:
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.float32, # Standard precision for CPU compatibility
        device_map="cpu"           # Strictly use CPU
    )

    print("--- Local AI Ready (Transformers Mode) ---")

    # Simple prompt
    prompt = "User: What is 2+2?\nAssistant:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate answer
    outputs = model.generate(**inputs, max_new_tokens=20)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"Response:\n{answer}")

except Exception as e:
    print(f"An error occurred: {e}")


================================================================================
File: app/core/llm_adapter.py
Size: 1.07 kB
================================================================================

from llama_cpp import Llama
import os

class LLMAdapter:
    def __init__(self, model_path="models/tinyllama.gguf"):
        # Explicit check for the model file
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found at {model_path}")
            
        # Optimization for older Macs: Force CPU only
        self.llm = Llama(
            model_path=model_path,
            n_ctx=512,      # Context window
            n_threads=4,    # Number of CPU cores to use
            n_gpu_layers=0, # <--- CRITICAL: Set to 0 to disable Metal/GPU
            verbose=False
        )

    def generate_response(self, prompt: str, max_tokens: int = 100) -> str:
        # Chat format for TinyLlama
        formatted_prompt = f"<|system|>\nUse the context to answer.</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n"
        
        output = self.llm(
            formatted_prompt,
            max_tokens=max_tokens,
            stop=["</s>"],
            echo=False
        )
        
        return output["choices"][0]["text"].strip()

================================================================================
File: app/embeddings/ingest.py
Size: 1.25 kB
================================================================================

import os
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

def run_ingestion():
    data_path = "data/"
    if not os.path.exists(data_path):
        os.makedirs(data_path)
        
    documents = []
    
    for file in os.listdir(data_path):
        file_path = os.path.join(data_path, file)
        if file.endswith(".txt"):
            documents.extend(TextLoader(file_path).load())
        elif file.endswith(".pdf"):
            documents.extend(PyPDFLoader(file_path).load())

    if not documents:
        print("No documents found in /data folder.")
        return

    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents(documents)
    
    # Using a free, local model for embeddings (no API key needed)
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    vectorstore.save_local("data/faiss_index")
    print("Ingestion complete. Index saved to data/faiss_index")

if __name__ == "__main__":
    run_ingestion()

================================================================================
File: app/main.py
Size: 2.83 kB
================================================================================

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import os

# Internal imports
from app.utils.redactor import PIIRedactor
from app.core.llm_adapter import LLMAdapter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

app = FastAPI(title="AI Co-Pilot API")

# --- CORS Configuration ---
# This allows your frontend (port 3000) to talk to your backend (port 8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, replace with ["http://localhost:3000"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize components once to optimize memory on your Mac
redactor = PIIRedactor()
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
# Ensure TinyLlama is ready via PyTorch 2.2.2
llm = LLMAdapter()

class QueryRequest(BaseModel):
    user_id: str
    query: str

@app.get("/health")
def health_check():
    """Service health check endpoint."""
    return {"status": "healthy", "model": "TinyLlama-1.1B"}

@app.post("/api/query")
async def process_query(request: QueryRequest):
    """
    Main RAG endpoint:
    1. Redacts PII
    2. Searches FAISS for context
    3. Generates response using Local LLM
    """
    # 1. Privacy Filter
    clean_query = redactor.redact(request.query)
    
    index_path = "data/faiss_index"
    if not os.path.exists(index_path):
        return {
            "query_id": "error",
            "answer": "Knowledge base not indexed yet. Please run ingest.py first.",
            "provenance": []
        }

    try:
        # 2. Retrieval (RAG)
        vectorstore = FAISS.load_local(
            index_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # Search for the top 3 relevant chunks
        docs = vectorstore.similarity_search(clean_query, k=3)
        context_text = " ".join([d.page_content for d in docs])
        sources = list(set([d.metadata.get('source', 'unknown') for d in docs]))
        
        # 3. Generation (Local LLM)
        # We build a prompt that forces the AI to use the retrieved context
        refined_prompt = f"Context: {context_text}\n\nQuestion: {clean_query}\n\nAnswer:"
        ai_response = llm.generate_response(refined_prompt)
        
        return {
            "query_id": "req-local-rag",
            "answer": ai_response,
            "provenance": sources
        }
        
    except Exception as e:
        print(f"Error during RAG flow: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error during generation.")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

================================================================================
File: app/utils/redactor.py
Size: 372 B
================================================================================

import re

class PIIRedactor:
    def __init__(self):
        self.email_pattern = re.compile(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+')
        self.phone_pattern = re.compile(r'\+?\d{10,12}')

    def redact(self, text: str) -> str:
        text = self.email_pattern.sub("[EMAIL]", text)
        text = self.phone_pattern.sub("[PHONE]", text)
        return text

================================================================================
File: app/utils/telemetry.py
Size: 920 B
================================================================================

import csv
import os
from datetime import datetime

METRICS_FILE = "metrics/usage_logs.csv"

class Telemetry:
    def __init__(self):
        os.makedirs("metrics", exist_ok=True)
        if not os.path.exists(METRICS_FILE):
            self._create_header()

    def _create_header(self):
        headers = ["timestamp", "user_id", "query_length", "latency_ms", "feedback_score"]
        with open(METRICS_FILE, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)

    def log_interaction(self, user_id, query, latency, feedback=None):
        with open(METRICS_FILE, "a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow([
                datetime.now().isoformat(),
                user_id,
                len(query),
                latency,
                feedback if feedback else "N/A"
            ])

================================================================================
File: data/main.py
Size: 959 B
================================================================================

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import datetime
import csv

app = FastAPI(title="AI Co-Pilot API")

class QueryRequest(BaseModel):
    user_id: str
    query: str

class FeedbackRequest(BaseModel):
    query_id: str
    rating: int
    comment: Optional[str] = None

@app.get("/health")
def health_check():
    return {"status": "healthy", "timestamp": datetime.datetime.now().isoformat()}

@app.post("/api/query")
async def process_query(request: QueryRequest):
    return {
        "query_id": "req-123",
        "answer": "This is a placeholder response for the RAG flow.",
        "provenance": ["doc_1.pdf", "doc_2.txt"]
    }

@app.post("/api/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    return {"status": "success", "message": "Feedback received"}

@app.get("/api/metrics")
async def export_metrics():
    return {"download_url": "/metrics/export.csv"}

================================================================================
File: data/test_info.txt
Size: 213 B
================================================================================

"The AI Co-Pilot Adoption project started in February 2026. Our main goal is to automate internal documentation tasks using FastAPI and OpenAI. We are currently working on the ingestion phase using a MacBook Air."

================================================================================
File: docker-compose.yml
Size: 443 B
================================================================================

version: "3.8"

services:
  backend:
    build:
      context: ./app
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./app:/app
      - ./data:/app/data
    networks:
      - ai-network

  frontend:
    image: nginx:alpine
    ports:
      - "3000:80"
    volumes:
      - ./frontend:/usr/share/nginx/html
    networks:
      - ai-network

networks:
  ai-network:
    driver: bridge


================================================================================
File: docs/adoption_plan.md
Size: 740 B
================================================================================

# Adoption Plan & Learning Path

## 1. Champions Program

- [cite_start]**Selection**: Identify 10-30 power users from different departments[cite: 170].
- [cite_start]**Role**: Act as first-line support and gather internal use cases[cite: 170].

## 2. Learning Path

- [cite_start]**Phase 1: Awareness**: 30-minute executive demo and intro webinar[cite: 167].
- [cite_start]**Phase 2: Hands-On**: Two 60-minute guided labs focusing on RAG and Summarization[cite: 168].
- [cite_start]**Phase 3: Proficiency**: Weekly drop-in "Office Hours" for 4 weeks[cite: 169].

## 3. Communication Strategy

- **Kickoff**: Company-wide email from leadership.
- [cite_start]**Success Stories**: Weekly newsletter highlighting a "Champion Win"[cite: 171].


================================================================================
File: docs/architecture.md
Size: 677 B
================================================================================

# Architecture Documentation

## System Overview

[cite_start]This project implements a Retrieval-Augmented Generation (RAG) architecture designed for local reproduction and pilot testing[cite: 2, 80].

## Component Diagram

[cite_start]The diagram below illustrates the data flow from the user to the LLM and the telemetry system[cite: 6, 126, 132].

```mermaid
flowchart TD
    User((User)) --> UI[Frontend / Slack]
    UI --> API[FastAPI Backend]
    API --> Redactor[PII Redactor Module]
    Redactor --> VDB[(FAISS Vector DB)]
    VDB --> Context[Document Context]
    Context --> LLM{LLM Engine}
    LLM --> API
    API --> Metrics[Telemetry Pipeline]
    API --> UI
```


================================================================================
File: docs/governance_dsgvo.md
Size: 1.02 kB
================================================================================

# KI-Governance & Datenschutz (DSGVO)

## 1. Local-First Approach

Die Verarbeitung erfolgt prim√§r lokal auf macOS-Infrastruktur. Dies minimiert das Risiko von Datenabfluss an Drittanbieter.

## 2. Risikomatrix

| Risiko              | Auswirkung | Mitigierung                                                            |
| :------------------ | :--------- | :--------------------------------------------------------------------- |
| **Datenschutz**     | Hoch       | Einsatz von PII-Redaction (Anonymisierung) vor jeder LLM-Verarbeitung. |
| **Halluzinationen** | Mittel     | RAG-Einsatz: Antworten sind durch lokale Dokumente (FAISS) belegt.     |
| **Sicherheit**      | Hoch       | Keine externe API-Abh√§ngigkeit im Standardmodus (Air-gapped m√∂glich).  |

## 3. DSGVO Checklist

- [x] Datenminimierung: Nur relevante Textfragmente werden verarbeitet.
- [x] Transparenz: Alle KI-Antworten werden als solche gekennzeichnet.
- [x] Recht auf L√∂schung: Lokale Indizes (FAISS) k√∂nnen jederzeit gel√∂scht werden.


================================================================================
File: docs/pitch_deck.md
Size: 1.25 kB
================================================================================

# Pitch Deck: AI Co-Pilot & Adoption Program

## Executive Summary

Strategic implementation of a private, local AI assistant to increase operational efficiency by 20% while maintaining 100% data sovereignty.

## 1. The Challenge

- **Information Overload**: Employees spend hours searching internal documentation.
- **Security Risks**: Using public AI (ChatGPT) exposes sensitive company data.
- **Adoption Barrier**: 60% of AI tools fail due to lack of user training.

## 2. Our Solution: AI Co-Pilot

- **Privacy by Design**: Runs locally on macOS. Zero data leaves the company.
- **Context-Aware**: Connected to our specific PDFs, TXT, and meeting notes.
- **Scalable**: Built with a modular adapter to switch models (TinyLlama to Llama 3).

## 3. Pilot Results (KPIs)

- **Accuracy**: High relevance through RAG (FAISS).
- **Speed**: Optimized for CPU (PyTorch 2.2.2).
- **User Sentiment**: 85% positive feedback in initial alpha tests.

## 4. Rollout Strategy (The "Human" Side)

- **Champions Program**: 20 power users to lead the transition.
- **Modular Learning**: 3-stage training (Awareness, Hands-on, Mastery).

## 5. Next Steps

- Phase 1: Deploy to the "Champions" group (Week 1).
- Phase 2: Full department integration (Month 2).


================================================================================
File: docs/rollout_plan.md
Size: 460 B
================================================================================

# Rollout & Communication Plan

## Woche 1: Soft Launch (Champions)

- **Ziel**: Feedback von 5 Key-Usern sammeln.
- **Kanal**: Direkte Slack-Gruppe.

## Woche 2: Training & Awareness

- **Event**: 45-min Webinar "KI im Arbeitsalltag sicher nutzen".
- **Doku**: Quick-Start Guide (PDF) an alle Teilnehmenden.

## Woche 3: Full Pilot

- **KPI Check**: Auswertung der Telemetrie (Latency vs. Feedback Score).
- **Support**: T√§gliche "Office Hours" f√ºr Fragen.


================================================================================
File: docs/user_guide.md
Size: 347 B
================================================================================

# Quick Start Guide for AI Co-Pilot Champions

## Welcome, Champion!

You have been selected to lead the AI transformation in your department. This tool helps you query internal knowledge securely.

## 1. How to Start the Assistant

Run the following command in your terminal:

```bash
source venv/bin/activate
python3 -m uvicorn app.main:app
```


================================================================================
File: frontend/index.html
Size: 1.73 kB
================================================================================

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Co-Pilot Demo</title>
    <style>
      body {
        font-family: sans-serif;
        margin: 2rem;
      }
      #chat {
        border: 1px solid #ccc;
        padding: 1rem;
        height: 300px;
        overflow-y: scroll;
        margin-bottom: 1rem;
      }
      .user {
        color: blue;
      }
      .ai {
        color: green;
      }
    </style>
  </head>
  <body>
    <h1>AI Co-Pilot PoC</h1>
    <div id="chat"></div>
    <input
      type="text"
      id="query"
      placeholder="Ask something..."
      style="width: 80%"
    />
    <button onclick="sendQuery()">Send</button>

    <script>
      async function sendQuery() {
        const query = document.getElementById("query").value;
        const chat = document.getElementById("chat");

        chat.innerHTML += `<p class="user"><b>You:</b> ${query}</p>`;

        const response = await fetch("http://localhost:8000/api/query", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ user_id: "demo-user", query: query }),
        });

        const data = await response.json();
        chat.innerHTML += `<p class="ai"><b>AI:</b> ${data.answer}</p>`;
        chat.innerHTML += `<small>Sources: ${data.provenance.join(
          ", "
        )}</small>`;
        chat.innerHTML += `<br><button onclick="sendFeedback('up')">üëç</button> <button onclick="sendFeedback('down')">üëé</button>`;
      }

      async function sendFeedback(type) {
        alert("Feedback sent: " + type);
      }
    </script>
  </body>
</html>


================================================================================
File: metrics/dashboard_mock.py
Size: 558 B
================================================================================

import pandas as pd

def generate_summary():
    try:
        df = pd.read_csv("metrics/usage_logs.csv")
        summary = {
            "Total Queries": len(df),
            "Avg Latency (ms)": df["latency_ms"].mean(),
            "Positive Feedback %": (len(df[df["feedback_score"] == "up"]) / len(df)) * 100
        }
        print("--- AI Co-Pilot KPI Summary ---")
        for k, v in summary.items():
            print(f"{k}: {v:.2f}")
    except Exception:
        print("No metrics data found yet.")

if __name__ == "__main__":
    generate_summary()

================================================================================
File: metrics/simulator.py
Size: 835 B
================================================================================

import csv
import random
import datetime
import os

def generate_mock_metrics(filename="metrics/export.csv"):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    headers = ["request_id", "timestamp", "user_hash", "intent", "latency_ms", "feedback"]
    intents = ["RAG", "Summary", "Action Extraction"]
    
    with open(filename, mode="w", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(headers)
        for i in range(50):
            writer.writerow([
                f"req-{i}",
                datetime.datetime.now().isoformat(),
                f"user-{random.randint(1, 10)}",
                random.choice(intents),
                random.randint(200, 3500),
                random.choice(["up", "down", "none"])
            ])

if __name__ == "__main__":
    generate_mock_metrics()

================================================================================
File: requirements.txt
Size: 229 B
================================================================================

fastapi==0.109.0
uvicorn==0.27.0
pydantic==2.5.3
langchain==0.1.0
langchain-community==0.0.13
langchain-huggingface==0.0.1
sentence-transformers==2.3.1
faiss-cpu==1.7.4
pypdf==4.0.1
torch==2.2.2
transformers==4.37.2
pandas==2.2.0

================================================================================
File: setup.sh
Size: 174 B
================================================================================

python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r app/requirements.txt
echo "Environment ready. Use 'source venv/bin/activate' to start."

================================================================================
File: test_final.py
Size: 910 B
================================================================================

import torch
from transformers import pipeline
import sys

print("--- System Diagnostics ---")
print(f"Python version: {sys.version.split()[0]}")
print(f"PyTorch version: {torch.__version__}")

try:
    print("\nLoading TinyLlama (1.1B)... this is a very compatible model.")
    # This model is around 700MB, much lighter for your RAM
    pipe = pipeline(
        "text-generation", 
        model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", 
        torch_dtype=torch.float32, 
        device_map="cpu"
    )

    print("\n‚úÖ Success! Local AI is active.")
    
    prompt = "<|system|>\nYou are a helpful assistant.</s>\n<|user|>\nWhat is 2+2?</s>\n<|assistant|>\n"
    
    outputs = pipe(prompt, max_new_tokens=20, do_sample=True, temperature=0.7)
    print(f"\nResponse: {outputs[0]['generated_text'].split('<|assistant|>')[-1].strip()}")

except Exception as e:
    print(f"\n‚ùå Error during execution: {e}")

================================================================================
File: test_local.py
Size: 968 B
================================================================================

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Define a small, efficient model (DeepSeek-1.5B)
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

print("Loading model into RAM... (this might take a few minutes)")

try:
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.float32, # Standard precision for CPU compatibility
        device_map="cpu"           # Strictly use CPU
    )

    print("--- Local AI Ready (Transformers Mode) ---")

    # Simple prompt
    prompt = "User: What is 2+2?\nAssistant:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate answer
    outputs = model.generate(**inputs, max_new_tokens=20)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"Response:\n{answer}")

except Exception as e:
    print(f"An error occurred: {e}")

================================================================================
File: test_rag.p
Size: 1.2 kB
================================================================================

import os
from app.core.llm_adapter import LLMAdapter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

def terminal_test():
    print("\n--- 1. Initializing Embeddings ---")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    print("--- 2. Loading FAISS Index ---")
    index_path = "data/faiss_index"
    if not os.path.exists(index_path):
        print("Error: Index not found.")
        return
    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
    print("--- 3. Searching for Context (RAG) ---")
    query = "When did the AI Co-Pilot project start?"
    docs = vectorstore.similarity_search(query, k=1)
    context = docs[0].page_content if docs else "No context found."
    print(f"Retrieved context: {context}")
    print("--- 4. Initializing TinyLlama (Loading RAM...) ---")
    llm = LLMAdapter()
    print("--- 5. Generating Answer (Final Step) ---")
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    response = llm.generate_response(prompt, max_tokens=30)
    print(f"\nAI RESPONSE: {response}\n")

if __name__ == "__main__":
    terminal_test()

================================================================================
File: test_rag.py
Size: 1.33 kB
================================================================================

import os
from app.core.llm_adapter import LLMAdapter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

def terminal_test():
    print("\n--- 1. Initializing Embeddings ---")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    print("--- 2. Loading FAISS Index ---")
    index_path = "data/faiss_index"
    if not os.path.exists(index_path):
        print("Error: Index not found. Run ingest.py first.")
        return
        
    vectorstore = FAISS.load_local(
        index_path, 
        embeddings, 
        allow_dangerous_deserialization=True
    )
    
    print("--- 3. Searching for Context (RAG) ---")
    query = "When did the AI Co-Pilot project start?"
    docs = vectorstore.similarity_search(query, k=1)
    context = docs[0].page_content if docs else "No context found."
    print(f"Retrieved context: {context}")

    print("--- 4. Initializing TinyLlama (Loading RAM...) ---")
    llm = LLMAdapter()
    
    print("--- 5. Generating Answer (Final Step) ---")
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    response = llm.generate_response(prompt, max_tokens=30)
    
    print("\n" + "="*30)
    print(f"AI RESPONSE: {response}")
    print("="*30 + "\n")

if __name__ == "__main__":
    terminal_test()



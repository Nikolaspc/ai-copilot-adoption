
================================================================================
File: .env.example
Size: 0 B
================================================================================



================================================================================
File: .flake8
Size: 129 B
================================================================================

# .flake8
[flake8]
max-line-length = 88
extend-ignore = E203
exclude = 
    .git,
    __pycache__,
    venv,
    .venv,
    docs/

================================================================================
File: .github/workflows/ci.yml
Size: 1 kB
================================================================================

# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest
          if [ -f app/requirements.txt ]; then pip install -r app/requirements.txt; fi

      - name: Lint with flake8
        run: flake8 .

      - name: Run unit tests
        run: |
          # We'll create a dummy test to ensure this passes
          pytest tests/

  docker-build:
    runs-on: ubuntu-latest
    needs: lint-and-test
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Build Backend Image
        run: docker build -t ai-copilot-backend:latest ./app


================================================================================
File: .gitignore
Size: 194 B
================================================================================

# Python
__pycache__/
*.py[cod]
*$py.class
.venv
venv/
ENV/

# Secrets and Environment
.env
.env.local

# IDEs
.vscode/
.idea/

# OS
.DS_Store

# Vector DB / Local data
data/index_storage/
*.csv

================================================================================
File: LICENSE
Size: 1.07 kB
================================================================================

MIT License

Copyright (c) 2026 Nikolaspc

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
File: app/embeddings/ingest.py
Size: 1.31 kB
================================================================================

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# Load environment variables (API Key)
load_dotenv()

def ingest_docs():
    # 1. Path to your documents folder
    data_path = "data/"
    documents = []

    # 2. Loop through files in data folder
    for file in os.listdir(data_path):
        if file.endswith(".txt"):
            loader = TextLoader(os.path.join(data_path, file))
            documents.extend(loader.load())
        elif file.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(data_path, file))
            documents.extend(loader.load())

    # 3. Split text into manageable chunks
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)

    # 4. Create embeddings and save to FAISS (Vector DB)
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)
    
    # 5. Save the index locally
    vectorstore.save_local("faiss_index")
    print("Successfully ingested documents into faiss_index")

if __name__ == "__main__":
    ingest_docs()

================================================================================
File: app/main.py
Size: 240 B
================================================================================

from fastapi import FastAPI

app = FastAPI(title="AI Co-Pilot API")

@app.get("/")
def read_root():
    return {"status": "AI Co-Pilot is online", "version": "0.1.0"}

@app.get("/health")
def health_check():
    return {"status": "healthy"}

================================================================================
File: data/test_info.txt
Size: 213 B
================================================================================

"The AI Co-Pilot Adoption project started in February 2026. Our main goal is to automate internal documentation tasks using FastAPI and OpenAI. We are currently working on the ingestion phase using a MacBook Air."

================================================================================
File: requirements.txt
Size: 0 B
================================================================================



================================================================================
File: test_final.py
Size: 910 B
================================================================================

import torch
from transformers import pipeline
import sys

print("--- System Diagnostics ---")
print(f"Python version: {sys.version.split()[0]}")
print(f"PyTorch version: {torch.__version__}")

try:
    print("\nLoading TinyLlama (1.1B)... this is a very compatible model.")
    # This model is around 700MB, much lighter for your RAM
    pipe = pipeline(
        "text-generation", 
        model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", 
        torch_dtype=torch.float32, 
        device_map="cpu"
    )

    print("\n✅ Success! Local AI is active.")
    
    prompt = "<|system|>\nYou are a helpful assistant.</s>\n<|user|>\nWhat is 2+2?</s>\n<|assistant|>\n"
    
    outputs = pipe(prompt, max_new_tokens=20, do_sample=True, temperature=0.7)
    print(f"\nResponse: {outputs[0]['generated_text'].split('<|assistant|>')[-1].strip()}")

except Exception as e:
    print(f"\n❌ Error during execution: {e}")

================================================================================
File: test_local.py
Size: 968 B
================================================================================

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Define a small, efficient model (DeepSeek-1.5B)
model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

print("Loading model into RAM... (this might take a few minutes)")

try:
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.float32, # Standard precision for CPU compatibility
        device_map="cpu"           # Strictly use CPU
    )

    print("--- Local AI Ready (Transformers Mode) ---")

    # Simple prompt
    prompt = "User: What is 2+2?\nAssistant:"
    inputs = tokenizer(prompt, return_tensors="pt")

    # Generate answer
    outputs = model.generate(**inputs, max_new_tokens=20)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"Response:\n{answer}")

except Exception as e:
    print(f"An error occurred: {e}")
